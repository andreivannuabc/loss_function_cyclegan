{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":886249,"sourceType":"datasetVersion","datasetId":472549},{"sourceId":4272422,"sourceType":"datasetVersion","datasetId":2517288},{"sourceId":8050050,"sourceType":"datasetVersion","datasetId":4747199},{"sourceId":9090405,"sourceType":"datasetVersion","datasetId":5485449},{"sourceId":9293099,"sourceType":"datasetVersion","datasetId":5626147},{"sourceId":9420510,"sourceType":"datasetVersion","datasetId":5721816},{"sourceId":9919562,"sourceType":"datasetVersion","datasetId":6096282},{"sourceId":9921427,"sourceType":"datasetVersion","datasetId":6097585},{"sourceId":9945613,"sourceType":"datasetVersion","datasetId":6115629},{"sourceId":9945769,"sourceType":"datasetVersion","datasetId":6115758},{"sourceId":9946405,"sourceType":"datasetVersion","datasetId":6116187},{"sourceId":9947235,"sourceType":"datasetVersion","datasetId":6116735},{"sourceId":10053605,"sourceType":"datasetVersion","datasetId":6194768},{"sourceId":10105100,"sourceType":"datasetVersion","datasetId":6233164},{"sourceId":10143851,"sourceType":"datasetVersion","datasetId":6261224},{"sourceId":10210432,"sourceType":"datasetVersion","datasetId":6310579},{"sourceId":10212764,"sourceType":"datasetVersion","datasetId":6312204},{"sourceId":10248898,"sourceType":"datasetVersion","datasetId":6339021},{"sourceId":10250044,"sourceType":"datasetVersion","datasetId":6339868},{"sourceId":10262628,"sourceType":"datasetVersion","datasetId":6348670},{"sourceId":10276136,"sourceType":"datasetVersion","datasetId":6358452},{"sourceId":10280802,"sourceType":"datasetVersion","datasetId":6361884},{"sourceId":10298879,"sourceType":"datasetVersion","datasetId":6374550},{"sourceId":10315405,"sourceType":"datasetVersion","datasetId":6386110},{"sourceId":10438727,"sourceType":"datasetVersion","datasetId":6462550},{"sourceId":10819980,"sourceType":"datasetVersion","datasetId":6717843},{"sourceId":10820693,"sourceType":"datasetVersion","datasetId":6718324},{"sourceId":10844494,"sourceType":"datasetVersion","datasetId":6734934},{"sourceId":10844517,"sourceType":"datasetVersion","datasetId":6734956}],"dockerImageVersionId":30589,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src='https://github.com/andreivannuabc/loss_function_cyclegan/blob/main/cycle_consistenc.png?raw=true' height=350></center>\n<p>\n   \n\n<h1><center> Perception Loss Function using CycleGan in Pathologic Myopia</center></h1>\n<h2><center> Introduction to a Novel Loss Function </center></h2>","metadata":{}},{"cell_type":"markdown","source":"<h1>Package</h1>","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import Model, losses, optimizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:27:07.402488Z","iopub.execute_input":"2025-02-24T22:27:07.403320Z","iopub.status.idle":"2025-02-24T22:27:21.161646Z","shell.execute_reply.started":"2025-02-24T22:27:07.403281Z","shell.execute_reply":"2025-02-24T22:27:21.160753Z"}},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1740436034.069944      12 common_lib.cc:815] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=local.\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:531\n/usr/local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"HEIGHT = 768\nWIDTH = 768 \nCHANNELS = 3\nEPOCHS = 200\nBATCH_SIZE = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:27:21.163481Z","iopub.execute_input":"2025-02-24T22:27:21.164027Z","iopub.status.idle":"2025-02-24T22:27:21.167980Z","shell.execute_reply.started":"2025-02-24T22:27:21.163992Z","shell.execute_reply":"2025-02-24T22:27:21.167266Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\n\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')\nAUTO = tf.data.experimental.AUTOTUNE\nAUTO = tf.data.AUTOTUNE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:27:21.168844Z","iopub.execute_input":"2025-02-24T22:27:21.169085Z","iopub.status.idle":"2025-02-24T22:27:29.940380Z","shell.execute_reply.started":"2025-02-24T22:27:21.169059Z","shell.execute_reply":"2025-02-24T22:27:29.939601Z"}},"outputs":[{"name":"stdout","text":"Running on TPU \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"2025-02-24 22:27:25.285692: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.285836: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.285913: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.285986: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.286056: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.286254: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.286354: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.286443: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.286547: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.286650: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.286884: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.287011: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.287107: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.287189: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.287299: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.287507: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.287618: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.287712: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.287807: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.287891: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.288084: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.288181: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.288272: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.288368: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.288458: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.288731: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.288833: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.288915: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.289008: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.289089: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.289361: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.289493: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.289599: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.289689: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.289783: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.290059: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.290163: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.290259: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.290355: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:27:25.290441: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use the non-experimental symbol `tf.distribute.TPUStrategy` instead.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS: 8\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport tensorflow as tf\nfrom PIL import Image\n\ndef get_filenames_from_folder(folder_path, extensions, limit=None):\n    filenames = []\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            if file.endswith(extensions):\n                file_path = os.path.join(root, file)\n                # Verificación opcional para asegurarse de que la imagen no esté corrupta\n                try:\n                    img = Image.open(file_path)\n                    img.verify()  # Verifica si la imagen está corrupta\n                    filenames.append(file_path)\n                    if limit and len(filenames) >= limit:\n                        return filenames\n                except (IOError, SyntaxError) as e:\n                    print(f\"Imagen corrupta detectada: {file_path}\")\n                    continue  # Ignora la imagen corrupta\n    return filenames\n\n# Aplica la función para obtener los nombres de archivo de la Clase 1\nclass_1_filenamesaptos = get_filenames_from_folder(\"/kaggle/input/stargan-retinas/stargan_retina/normal_fundus\", ('.jpg', '.jpeg', '.png', '.tif'), 5000)\nclass_1_filenamesodir = get_filenames_from_folder(\"/kaggle/input/dataset-normal-1024/normal\", ('.jpg', '.jpeg', '.png', '.tif'),6000)\nclass_1_filenames = class_1_filenamesodir \n#class_1_filenames_train = get_filenames_from_folder(\"/kaggle/input/indian-diabetic-retinopathy-image-dataset/A.%20Segmentation/A. Segmentation/1. Original Images/a. Training Set\", ('.jpg', '.jpeg', '.png', '.tif'), 81)\n#class_1_filenames_test = get_filenames_from_folder(\"/kaggle/input/indian-diabetic-retinopathy-image-dataset/A.%20Segmentation/A. Segmentation/1. Original Images/b. Testing Set\", ('.jpg', '.jpeg', '.png', '.tif'), 81)\nclass_2_filenamesa = get_filenames_from_folder(\"/kaggle/input/dataset-nohor-toxoplasmosis/todocropaug\", ('.jpg', '.jpeg', '.png', '.tif'), 9600)\nclass_2_filenamesb = get_filenames_from_folder(\"/kaggle/input/dataset-toxoplasmosis-hor/todocropaughor\", ('.jpg', '.jpeg', '.png', '.tif'), 9600)\n\n\n# Función para contar los elementos\ndef count_data_items(filenames):\n    return len(filenames)\n\n# Combinar las listas de archivos de la clase 2\nclass_2_filenames = class_2_filenamesa  + class_2_filenamesb\nn_class_1_samples = count_data_items(class_1_filenames)\nn_class_2_samples = count_data_items(class_2_filenames)\nprint(f'Número de archivos Clase normal fundus: {n_class_1_samples}')\nprint(f'Número de archivos Clase diabetic: {n_class_2_samples}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:27:29.942050Z","iopub.execute_input":"2025-02-24T22:27:29.942499Z","iopub.status.idle":"2025-02-24T22:29:59.836775Z","shell.execute_reply.started":"2025-02-24T22:27:29.942450Z","shell.execute_reply":"2025-02-24T22:29:59.835542Z"}},"outputs":[{"name":"stdout","text":"Número de archivos Clase normal fundus: 6000\nNúmero de archivos Clase diabetic: 7618\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef image_example(image_string):\n    feature = {\n        'image': _bytes_feature(image_string),\n    }\n    return tf.train.Example(features=tf.train.Features(feature=feature))\n\ndef write_tfrecord(image_paths, output_file):\n    with tf.io.TFRecordWriter(output_file) as writer:\n        for image_path in image_paths:\n            try:\n                with open(image_path, 'rb') as f:\n                    image_string = f.read()\n                if not image_string:\n                    print(f\"Imagen vacía: {image_path}\")\n                    continue\n                tf_example = image_example(image_string)\n                writer.write(tf_example.SerializeToString())\n            except Exception as e:\n                print(f\"Error al procesar {image_path}: {e}\")\n                continue\n\nwrite_tfrecord(class_1_filenames, 'class_1.tfrecord')\nwrite_tfrecord(class_2_filenames, 'class_2.tfrecord')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:29:59.838012Z","iopub.execute_input":"2025-02-24T22:29:59.838323Z","iopub.status.idle":"2025-02-24T22:30:16.473886Z","shell.execute_reply.started":"2025-02-24T22:29:59.838292Z","shell.execute_reply":"2025-02-24T22:30:16.472655Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def decode_image(image):\n    image = tf.image.decode_png(image, channels=CHANNELS)\n    image = tf.image.resize(image, [HEIGHT, WIDTH])  # Redimensionar la imagen\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    return image\n\n\ndef load_image(filename):\n    image = tf.io.read_file(filename)\n    image = decode_image(image)\n    return image\n\n\ndef parse_tfrecord(example_proto):\n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n    }\n    parsed_features = tf.io.parse_single_example(example_proto, image_feature_description)\n    image = tf.io.decode_image(parsed_features['image'], channels=CHANNELS, expand_animations=False)\n    image.set_shape([None, None, CHANNELS]) \n    image = tf.image.resize(image, [HEIGHT, WIDTH])\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    return image\n\n\ndef load_dataset(tfrecord_file):\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_file)\n    dataset = raw_dataset.map(parse_tfrecord, num_parallel_calls=AUTO)\n    return dataset\n\n\ndef display_samples(ds, row, col):\n    ds_iter = iter(ds)\n    plt.figure(figsize=(15, int(15*row/col)))\n    for j in range(row*col):\n        example_batch = next(ds_iter)\n        example_sample = example_batch \n        plt.subplot(row, col, j+1)\n        plt.axis('off')\n        plt.imshow((example_sample.numpy() * 0.5 + 0.5))\n    plt.show()\n\n    \ndef get_gan_dataset(monet_tfrecord, photo_tfrecord, batch_size=1):\n    monet_ds = load_dataset(monet_tfrecord)\n    photo_ds = load_dataset(photo_tfrecord)\n\n    monet_ds = monet_ds.cache().shuffle(1024).repeat()\n    photo_ds = photo_ds.cache().shuffle(1024).repeat()\n\n    monet_ds = monet_ds.batch(batch_size, drop_remainder=True)\n    photo_ds = photo_ds.batch(batch_size, drop_remainder=True)\n\n    monet_ds = monet_ds.prefetch(AUTO)\n    photo_ds = photo_ds.prefetch(AUTO)\n\n    gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))\n    return gan_ds\n\n\n\ndef display_generated_samples(ds, model, n_samples):\n    ds_iter = iter(ds)\n    for n_sample in range(n_samples):\n        example_sample = next(ds_iter)\n        generated_sample = model.predict(example_sample)\n        \n        plt.subplot(121)\n        plt.title(\"input image\")\n        plt.imshow(example_sample[0] * 0.5 + 0.5)\n        plt.axis('off')\n        \n        plt.subplot(122)\n        plt.title(\"Generated image\")\n        plt.imshow(generated_sample[0] * 0.5 + 0.5)\n        plt.axis('off')\n        plt.show()\n        \ndef predict_and_save(input_ds, model, output_path, domain_labels):\n    i = 1\n    for img, label in input_ds:\n        prediction = model([img, label], training=False)[0].numpy()\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n        im = PIL.Image.fromarray(prediction)\n        im.save(f'{output_path}{str(i)}.jpg')\n        i += 1\n\n\ndef downsample(filters, size, apply_instancenorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(L.Conv2D(filters, size, strides=2, padding='same',\n                        kernel_initializer=initializer, use_bias=False))\n    if apply_instancenorm:\n        result.add(tfa.layers.InstanceNormalization())\n    result.add(L.LeakyReLU())\n    return result\n\ndef upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(L.Conv2DTranspose(filters, size, strides=2,\n                                 padding='same',\n                                 kernel_initializer=initializer,\n                                 use_bias=False))\n    result.add(tfa.layers.InstanceNormalization())\n    result.add(L.ReLU())\n    if apply_dropout:\n        result.add(L.Dropout(0.5))\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:30:16.475124Z","iopub.execute_input":"2025-02-24T22:30:16.475424Z","iopub.status.idle":"2025-02-24T22:30:16.492539Z","shell.execute_reply.started":"2025-02-24T22:30:16.475396Z","shell.execute_reply":"2025-02-24T22:30:16.491776Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3\n# Asumiendo que HEIGHT, WIDTH, y CHANNELS están definidos\n# Ejemplo: HEIGHT = 256, WIDTH = 256, CHANNELS = 3\n\n\ndef generator_fn():\n    inputs = L.Input(shape=[HEIGHT, WIDTH, CHANNELS])\n\n    down_stack = [\n        downsample(64, 4, apply_instancenorm=False),\n        downsample(128, 4),\n        downsample(256, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n    ]\n\n    up_stack = [\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4),\n        upsample(512, 4),\n        upsample(256, 4),\n        upsample(128, 4),\n        upsample(64, 4),\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = L.Conv2DTranspose(OUTPUT_CHANNELS, 4, strides=2, padding='same',\n                             kernel_initializer=initializer, activation='tanh')\n\n    x = inputs\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    skips = list(reversed(skips[:-1]))\n\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2]:\n            x = tf.image.resize(x, (skip.shape[1], skip.shape[2]), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        x = L.Concatenate()([x, skip])\n\n    x = last(x)\n    return tf.keras.Model(inputs=inputs, outputs=x)\n\ndef discriminator_fn():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    inp = L.Input(shape=[HEIGHT, WIDTH, CHANNELS], name='input_image')\n\n    x = inp\n    down1 = downsample(64, 4, False)(x)\n    down2 = downsample(128, 4)(down1)\n    down3 = downsample(256, 4)(down2)\n    down4 = downsample(512, 4)(down3)\n\n    zero_pad1 = L.ZeroPadding2D()(down4)\n    conv = L.Conv2D(512, 3, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1)\n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n    leaky_relu = L.LeakyReLU()(norm1)\n    zero_pad2 = L.ZeroPadding2D()(leaky_relu)\n    last = L.Conv2D(1, 3, strides=1, kernel_initializer=initializer)(zero_pad2)\n\n    return Model(inputs=inp, outputs=last)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:30:16.493523Z","iopub.execute_input":"2025-02-24T22:30:16.493779Z","iopub.status.idle":"2025-02-24T22:30:19.550091Z","shell.execute_reply.started":"2025-02-24T22:30:16.493754Z","shell.execute_reply":"2025-02-24T22:30:19.549016Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss = losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.ones_like(real), real)\n        generated_loss = losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n        total_disc_loss = real_loss + generated_loss\n        return total_disc_loss * 0.5\n\n    def generator_loss(generated):\n        return losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.ones_like(generated), generated)\n    \n    with strategy.scope():\n        def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n            loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n            return LAMBDA * loss1\n\n    with strategy.scope():\n        def identity_loss(real_image, same_image, LAMBDA):\n            loss = tf.reduce_mean(tf.abs(real_image - same_image))\n            return LAMBDA * 0.5 * loss\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:30:19.551275Z","iopub.execute_input":"2025-02-24T22:30:19.551547Z","iopub.status.idle":"2025-02-24T22:30:29.542293Z","shell.execute_reply.started":"2025-02-24T22:30:19.551521Z","shell.execute_reply":"2025-02-24T22:30:29.541245Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG19\nfrom tensorflow.keras import layers\n\ndef model_build(image_size, num_classes):\n    inputs = tf.keras.Input(shape=(image_size, image_size, 3))\n    base_model = VGG19(weights='imagenet', include_top=False, input_tensor=inputs)\n    base_model.trainable = False\n    \n    x = base_model.output\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(64, activation='relu')(x)\n    x = layers.BatchNormalization()(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\ncnn_model = model_build(224, 2)\ncnn_model.load_weights('/kaggle/input/modelo-extraccion-macular/model_macular_extractlayer.h5')\ncnn_model.trainable = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:30:29.543515Z","iopub.execute_input":"2025-02-24T22:30:29.543821Z","iopub.status.idle":"2025-02-24T22:30:46.208859Z","shell.execute_reply.started":"2025-02-24T22:30:29.543793Z","shell.execute_reply":"2025-02-24T22:30:46.207731Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80134624/80134624 [==============================] - 2s 0us/step\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"cnn_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:30:46.211745Z","iopub.execute_input":"2025-02-24T22:30:46.212039Z","iopub.status.idle":"2025-02-24T22:30:52.262232Z","shell.execute_reply.started":"2025-02-24T22:30:46.212008Z","shell.execute_reply":"2025-02-24T22:30:52.261374Z"}},"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n                                                                 \n block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n                                                                 \n block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n                                                                 \n block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n                                                                 \n block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n                                                                 \n block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n                                                                 \n block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n                                                                 \n block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n                                                                 \n block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n                                                                 \n block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n                                                                 \n block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n                                                                 \n block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n                                                                 \n block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n                                                                 \n block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n                                                                 \n block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n                                                                 \n global_average_pooling2d (  (None, 512)               0         \n GlobalAveragePooling2D)                                         \n                                                                 \n dense (Dense)               (None, 64)                32832     \n                                                                 \n batch_normalization (Batch  (None, 64)                256       \n Normalization)                                                  \n                                                                 \n dense_1 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 20057602 (76.51 MB)\nTrainable params: 0 (0.00 Byte)\nNon-trainable params: 20057602 (76.51 MB)\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"layers_dict = {'0': 'conv1_1',\n               '5': 'conv2_1', \n               '10': 'conv3_1', \n               '19': 'conv4_1',\n               '21': 'conv4_2', \n               '28': 'conv5_1'}\n\nstyle_weights = {'conv1_1': 1.0,\n                 'conv2_1': 0.75,\n                 'conv3_1': 0.2,\n                 'conv4_1': 0.2,\n                 'conv5_1': 0.2}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:30:52.263257Z","iopub.execute_input":"2025-02-24T22:30:52.263532Z","iopub.status.idle":"2025-02-24T22:30:53.266737Z","shell.execute_reply.started":"2025-02-24T22:30:52.263504Z","shell.execute_reply":"2025-02-24T22:30:53.265892Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"layers_dict = {\n    'block2_conv1': 'conv2_1',\n    'block3_conv1': 'conv2_1',\n\n}\nstyle_weights = {\n    'conv1_2': 1.0,\n    'conv2_1': 0.75,\n    'conv3_1': 0.2,\n    'conv4_1': 0.2,\n    'conv4_2': 0.2,  # Agregar esta línea si quieres usar conv4_2\n    'conv5_1': 0.2\n}\n\ndef get_feature_extractor(cnn_model, layer_dict):\n    outputs = []\n    for layer_name in layer_dict.keys():\n        layer = cnn_model.get_layer(name=layer_name)\n        outputs.append(layer.output)\n    model = tf.keras.Model(inputs=cnn_model.input, outputs=outputs)\n    model.trainable = False\n    return model\n\nfeature_extractor = get_feature_extractor(cnn_model, layers_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:30:53.267667Z","iopub.execute_input":"2025-02-24T22:30:53.267902Z","iopub.status.idle":"2025-02-24T22:30:53.335841Z","shell.execute_reply.started":"2025-02-24T22:30:53.267878Z","shell.execute_reply":"2025-02-24T22:30:53.335091Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def hybrid_perceptual_loss(\n    real_images,\n    generated_images,\n    feature_extractor,  \n    layers_dict,        \n    style_weights,\n    threshold=0.5    \n):\n   \n\n    \n    real_images     = (real_images + 1.0)/2.0\n    generated_images= (generated_images + 1.0)/2.0\n\n  \n    mean = tf.constant([0.485, 0.456, 0.406], shape=(1,1,1,3), dtype=tf.float32)\n    std  = tf.constant([0.229, 0.224, 0.225], shape=(1,1,1,3), dtype=tf.float32)\n\n    real_images     = (real_images - mean)/std\n    generated_images= (generated_images - mean)/std\n\n  \n    real_images     = tf.image.resize(real_images, (224,224))\n    generated_images= tf.image.resize(generated_images, (224,224))\n\n   \n    real_features      = feature_extractor(real_images, training=False)\n    generated_features = feature_extractor(generated_images, training=False)\n\n    feature_names = list(layers_dict.values())\n    total_loss = 0.0\n\n    for real_f, gen_f, name in zip(real_features, generated_features, feature_names):\n  \n        shape = tf.shape(real_f)\n        C = tf.cast(shape[3], tf.float32)\n        H = tf.cast(shape[1], tf.float32)\n        W = tf.cast(shape[2], tf.float32)\n\n      \n        diff_sq = (gen_f - real_f)**2\n\n      \n        mask = tf.cast(real_f > threshold, tf.float32)  \n\n       \n        term_pathology = - diff_sq * mask\n        term_sana      = diff_sq * (1.0 - mask)\n        layer_loss = tf.reduce_mean(term_pathology + term_sana)\n\n       \n        layer_loss = layer_loss / (C * H * W)\n\n      \n        lambda_l = style_weights[name]\n        layer_loss *= lambda_l\n\n        total_loss += layer_loss\n\n    return total_loss\n\n\nlog_dir = './logs/gradient_logs'\nif not os.path.exists(log_dir):\n    os.makedirs(log_dir)\nfile_writer = tf.summary.create_file_writer(log_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:30:53.336776Z","iopub.execute_input":"2025-02-24T22:30:53.337025Z","iopub.status.idle":"2025-02-24T22:30:53.427786Z","shell.execute_reply.started":"2025-02-24T22:30:53.336999Z","shell.execute_reply":"2025-02-24T22:30:53.426934Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator = generator_fn() \n    photo_generator = generator_fn()\n\n    monet_discriminator = discriminator_fn()\n    photo_discriminator = discriminator_fn() \n\n\n\nclass CycleGan(Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        self.current_epoch = 0\n        self.global_step = tf.Variable(0, trainable=False, dtype=tf.int64)\n        self.summary_writer = file_writer\n\n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn,\n        perceptual_loss_fn=None,\n        layers_dict=None,\n        style_weights=None,\n        perceptual_lambda=0.1\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        self.perceptual_loss_fn = perceptual_loss_fn\n        self.layers_dict = layers_dict\n        self.style_weights = style_weights\n        self.perceptual_lambda = perceptual_lambda\n\n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n    \n        with tf.GradientTape(persistent=True) as tape:\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n    \n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n    \n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n    \n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + \\\n                               self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n            monet_id_loss = self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            photo_id_loss = self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n    \n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + monet_id_loss\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + photo_id_loss\n            \"\"\"\n            if self.perceptual_loss_fn is not None:\n                perc_loss_monet = self.perceptual_loss_fn(\n                    real_monet, \n                    fake_monet, \n                    feature_extractor, \n                    self.layers_dict, \n                    self.style_weights,\n                     threshold=0.5 \n                )\n                perc_loss_photo = self.perceptual_loss_fn(\n                    real_photo, \n                    fake_photo, \n                    feature_extractor, \n                    self.layers_dict, \n                    self.style_weights,\n                     threshold=0.5 \n                )\n                total_monet_gen_loss += self.perceptual_lambda * perc_loss_monet\n                total_photo_gen_loss += self.perceptual_lambda * perc_loss_photo\n               \"\"\"\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n    \n        monet_generator_gradients = tape.gradient(total_monet_gen_loss, self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss, self.p_gen.trainable_variables)\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss, self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss, self.p_disc.trainable_variables)\n    \n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients, self.m_gen.trainable_variables))\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients, self.p_gen.trainable_variables))\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients, self.m_disc.trainable_variables))\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients, self.p_disc.trainable_variables))\n    \n        self.global_step.assign_add(1)\n    \n        # Asegúrate que las métricas retornadas sean escalares (usamos tf.reduce_mean)\n        return {\n            'monet_gen_loss': tf.reduce_mean(total_monet_gen_loss),\n            'photo_gen_loss': tf.reduce_mean(total_photo_gen_loss),\n            'monet_disc_loss': tf.reduce_mean(monet_disc_loss),\n            'photo_disc_loss': tf.reduce_mean(photo_disc_loss)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:30:53.428967Z","iopub.execute_input":"2025-02-24T22:30:53.429241Z","iopub.status.idle":"2025-02-24T22:31:06.447162Z","shell.execute_reply.started":"2025-02-24T22:30:53.429205Z","shell.execute_reply":"2025-02-24T22:31:06.446332Z"}},"outputs":[{"name":"stderr","text":"2025-02-24 22:30:54.227364: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:31:05.609717: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"\nclass LearningRateLoggerCallback(tf.keras.callbacks.Callback):\n    def __init__(self, csv_file='lr_log.csv', monitor='monet_gen_loss'):\n        super().__init__()\n        self.csv_file = csv_file\n        self.monitor = monitor\n        self.prev_lrs = None\n        self.file_initialized = False\n\n    def on_train_begin(self, logs=None):\n     \n        with open(self.csv_file, 'w') as f:\n            f.write(\"epoch,\"+self.monitor+\",m_gen_lr,p_gen_lr,m_disc_lr,p_disc_lr\\n\")\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        metric_value = logs.get(self.monitor, None)\n\n      \n        m_gen_lr = float(tf.keras.backend.get_value(self.model.m_gen_optimizer.lr))\n        p_gen_lr = float(tf.keras.backend.get_value(self.model.p_gen_optimizer.lr))\n        m_disc_lr = float(tf.keras.backend.get_value(self.model.m_disc_optimizer.lr))\n        p_disc_lr = float(tf.keras.backend.get_value(self.model.p_disc_optimizer.lr))\n\n   \n        with open(self.csv_file, 'a') as f:\n            f.write(f\"{epoch},{metric_value},{m_gen_lr},{p_gen_lr},{m_disc_lr},{p_disc_lr}\\n\")\n\n       \n        current_lrs = (m_gen_lr, p_gen_lr, m_disc_lr, p_disc_lr)\n        if self.prev_lrs is not None:\n            \n            if current_lrs != self.prev_lrs:\n                print(f\"Epoch {epoch}: ¡Los learning rates han cambiado!\")\n                print(f\"Antes: {self.prev_lrs}, Ahora: {current_lrs}\")\n        self.prev_lrs = current_lrs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:31:06.448107Z","iopub.execute_input":"2025-02-24T22:31:06.448351Z","iopub.status.idle":"2025-02-24T22:31:06.456100Z","shell.execute_reply.started":"2025-02-24T22:31:06.448325Z","shell.execute_reply":"2025-02-24T22:31:06.455436Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def load_image(filepath, new_size=(224,224)):\n\n    img_bytes = tf.io.read_file(filepath)\n    img = tf.image.decode_jpeg(img_bytes, channels=3)\n    img = tf.image.resize(img, new_size)\n    img = tf.cast(img, tf.float32) / 127.5 - 1.0 \n    img = tf.expand_dims(img, axis=0)  \n    return img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:31:06.457003Z","iopub.execute_input":"2025-02-24T22:31:06.457245Z","iopub.status.idle":"2025-02-24T22:31:06.471865Z","shell.execute_reply.started":"2025-02-24T22:31:06.457209Z","shell.execute_reply":"2025-02-24T22:31:06.471209Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\nclass DisplaySampleCallback(tf.keras.callbacks.Callback):\n    def __init__(self, image_path, monet_generator, output_dir='./sample_epochs', freq=1):\n        super().__init__()\n        self.image_path = image_path\n        self.monet_generator = monet_generator\n        self.freq = freq\n        self.output_dir = output_dir\n\n        \n        if not os.path.exists(self.output_dir):\n            os.makedirs(self.output_dir)\n\n        \n        self.sample_photo = self._load_and_preprocess_image(self.image_path)\n\n    def _load_and_preprocess_image(self, image_path):\n     \n        img = Image.open(image_path).convert('RGB')  \n        img = img.resize((768, 768)) \n        img_array = np.array(img).astype('float32') / 127.5 - 1.0  \n        return tf.expand_dims(img_array, axis=0)  \n\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.freq == 0:\n          \n            generated_monet = self.monet_generator(self.sample_photo, training=False)\n            gen_img = (generated_monet[0].numpy() * 0.5) + 0.5 \n\n     \n            plt.figure(figsize=(6, 6))\n            plt.imshow(gen_img)\n            plt.axis('off')\n            plt.title(f\"Epoch {epoch + 1}\")\n            plt.savefig(os.path.join(self.output_dir, f\"epoch_{epoch + 1}.png\"))\n            plt.close()\n\n\nclass CustomTrainCheckpoints(tf.keras.callbacks.Callback):\n  \n    def __init__(self, checkpoint_dir='checkpoints', save_freq=5):\n       \n        super().__init__()\n        self.checkpoint_dir = checkpoint_dir\n        self.save_freq = save_freq\n        if not os.path.exists(self.checkpoint_dir):\n            os.makedirs(self.checkpoint_dir)\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.save_freq == 0:\n         \n            ckpt = tf.train.Checkpoint(\n                monet_generator=self.model.m_gen,\n                photo_generator=self.model.p_gen,\n                monet_discriminator=self.model.m_disc,\n                photo_discriminator=self.model.p_disc,\n                monet_gen_optimizer=self.model.m_gen_optimizer,\n                photo_gen_optimizer=self.model.p_gen_optimizer,\n                monet_disc_optimizer=self.model.m_disc_optimizer,\n                photo_disc_optimizer=self.model.p_disc_optimizer\n            )\n            ckpt_path = os.path.join(self.checkpoint_dir, f'ckpt_epoch_{epoch+1}')\n            ckpt.save(file_prefix=ckpt_path)\n            print(f\"Checkpoint guardado: {ckpt_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:31:06.472753Z","iopub.execute_input":"2025-02-24T22:31:06.473021Z","iopub.status.idle":"2025-02-24T22:31:06.487953Z","shell.execute_reply.started":"2025-02-24T22:31:06.472994Z","shell.execute_reply":"2025-02-24T22:31:06.487352Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"gan_ds = get_gan_dataset('class_2.tfrecord', 'class_1.tfrecord', batch_size=BATCH_SIZE)\nsteps_per_epoch = n_class_2_samples // BATCH_SIZE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:31:06.488759Z","iopub.execute_input":"2025-02-24T22:31:06.488986Z","iopub.status.idle":"2025-02-24T22:31:06.613995Z","shell.execute_reply.started":"2025-02-24T22:31:06.488963Z","shell.execute_reply":"2025-02-24T22:31:06.613208Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\nlog_dir = './logs/gradient_logs'\nif not os.path.exists(log_dir):\n    os.makedirs(log_dir)\nfile_writer = tf.summary.create_file_writer(log_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:31:06.614942Z","iopub.execute_input":"2025-02-24T22:31:06.615189Z","iopub.status.idle":"2025-02-24T22:31:06.619436Z","shell.execute_reply.started":"2025-02-24T22:31:06.615167Z","shell.execute_reply":"2025-02-24T22:31:06.618816Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:31:06.620190Z","iopub.execute_input":"2025-02-24T22:31:06.620428Z","iopub.status.idle":"2025-02-24T22:31:06.629089Z","shell.execute_reply.started":"2025-02-24T22:31:06.620405Z","shell.execute_reply":"2025-02-24T22:31:06.628472Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nwith strategy.scope():\n  \n    monet_generator = generator_fn()\n    photo_generator = generator_fn()\n    monet_discriminator = discriminator_fn()\n    photo_discriminator = discriminator_fn()\n\n  \n    monet_generator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n    monet_discriminator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n\n\n    gan_model = CycleGan(\n        monet_generator, \n        photo_generator,\n        monet_discriminator, \n        photo_discriminator,\n        lambda_cycle=10\n    )\n  \n    gan_model.compile(\n        m_gen_optimizer=monet_generator_optimizer,\n        p_gen_optimizer=photo_generator_optimizer,\n        m_disc_optimizer=monet_discriminator_optimizer,\n        p_disc_optimizer=photo_discriminator_optimizer,\n        gen_loss_fn=generator_loss,\n        disc_loss_fn=discriminator_loss,\n        cycle_loss_fn=calc_cycle_loss,\n        identity_loss_fn=identity_loss,\n        perceptual_loss_fn=hybrid_perceptual_loss,\n        layers_dict=layers_dict,\n        style_weights=style_weights,\n        perceptual_lambda=0.1\n    )\n\n    # 4. Callbacks\n    display_callback = DisplaySampleCallback(\n        image_path='/kaggle/input/dataset-normal-1024/normal/002c2135.jpg',\n        monet_generator=gan_model.m_gen,     \n        output_dir='./sample_epochs',    \n        freq=1                              \n    )\n    reduce_lr_callback = ReduceLROnPlateau(\n        monitor='monet_gen_loss',\n        factor=0.5,\n        patience=5,\n        verbose=1,\n        mode='min'\n    )\n\n    checkpoint_callback = CustomTrainCheckpoints(\n        checkpoint_dir='checkpoints',\n        save_freq=25\n    )\n\n\n    history = gan_model.fit(\n        gan_ds,\n        steps_per_epoch=steps_per_epoch,\n        epochs=200,\n        verbose=2,\n        callbacks=[display_callback, reduce_lr_callback, checkpoint_callback]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T22:31:06.630067Z","iopub.execute_input":"2025-02-24T22:31:06.630304Z","execution_failed":"2025-02-25T03:35:20.307Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"2025-02-24 22:31:13.430933: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:31:13.448253: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:31:13.465966: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:31:13.482462: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:31:13.499260: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:31:13.516345: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:31:13.533931: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:31:49.816023: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n2025-02-24 22:32:58.750626: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:32:58.751371: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:32:58.751495: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:32:58.751614: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:32:58.751710: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:32:58.751828: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:32:58.751915: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n2025-02-24 22:32:58.752042: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n","output_type":"stream"},{"name":"stdout","text":"238/238 - 283s - monet_gen_loss: 1.9354 - photo_gen_loss: 1.9564 - monet_disc_loss: 0.7242 - photo_disc_loss: 0.4723 - lr: 0.0010 - 283s/epoch - 1s/step\nEpoch 2/200\n238/238 - 177s - monet_gen_loss: 2.0131 - photo_gen_loss: 1.5677 - monet_disc_loss: 0.7205 - photo_disc_loss: 0.7491 - lr: 0.0010 - 177s/epoch - 745ms/step\nEpoch 3/200\n238/238 - 177s - monet_gen_loss: 1.7217 - photo_gen_loss: 1.6029 - monet_disc_loss: 0.6363 - photo_disc_loss: 0.6281 - lr: 0.0010 - 177s/epoch - 746ms/step\nEpoch 4/200\n238/238 - 177s - monet_gen_loss: 1.5900 - photo_gen_loss: 2.1965 - monet_disc_loss: 0.4816 - photo_disc_loss: 0.5178 - lr: 0.0010 - 177s/epoch - 745ms/step\nEpoch 5/200\n238/238 - 177s - monet_gen_loss: 1.6350 - photo_gen_loss: 1.6893 - monet_disc_loss: 0.6036 - photo_disc_loss: 0.6894 - lr: 0.0010 - 177s/epoch - 745ms/step\nEpoch 6/200\n238/238 - 177s - monet_gen_loss: 1.9049 - photo_gen_loss: 2.0192 - monet_disc_loss: 0.5910 - photo_disc_loss: 0.5724 - lr: 0.0010 - 177s/epoch - 745ms/step\nEpoch 7/200\n238/238 - 177s - monet_gen_loss: 1.7410 - photo_gen_loss: 1.9253 - monet_disc_loss: 0.6339 - photo_disc_loss: 0.5273 - lr: 0.0010 - 177s/epoch - 745ms/step\nEpoch 8/200\n238/238 - 178s - monet_gen_loss: 1.8663 - photo_gen_loss: 1.4705 - monet_disc_loss: 0.7118 - photo_disc_loss: 0.6792 - lr: 0.0010 - 178s/epoch - 746ms/step\nEpoch 9/200\n238/238 - 177s - monet_gen_loss: 1.2337 - photo_gen_loss: 1.4124 - monet_disc_loss: 0.6777 - photo_disc_loss: 0.5724 - lr: 0.0010 - 177s/epoch - 745ms/step\nEpoch 10/200\n238/238 - 178s - monet_gen_loss: 1.5420 - photo_gen_loss: 1.5523 - monet_disc_loss: 0.5616 - photo_disc_loss: 0.5525 - lr: 0.0010 - 178s/epoch - 746ms/step\nEpoch 11/200\n238/238 - 177s - monet_gen_loss: 1.4855 - photo_gen_loss: 1.7942 - monet_disc_loss: 0.6788 - photo_disc_loss: 0.7460 - lr: 0.0010 - 177s/epoch - 745ms/step\nEpoch 12/200\n238/238 - 177s - monet_gen_loss: 1.5108 - photo_gen_loss: 1.3432 - monet_disc_loss: 0.6778 - photo_disc_loss: 0.6673 - lr: 0.0010 - 177s/epoch - 745ms/step\nEpoch 13/200\n238/238 - 177s - monet_gen_loss: 1.3483 - photo_gen_loss: 1.4863 - monet_disc_loss: 0.6611 - photo_disc_loss: 0.6206 - lr: 0.0010 - 177s/epoch - 745ms/step\nEpoch 14/200\n\nEpoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n238/238 - 177s - monet_gen_loss: 1.5458 - photo_gen_loss: 1.1868 - monet_disc_loss: 0.5915 - photo_disc_loss: 0.8002 - lr: 0.0010 - 177s/epoch - 745ms/step\nEpoch 15/200\n238/238 - 178s - monet_gen_loss: 1.3923 - photo_gen_loss: 1.3283 - monet_disc_loss: 0.6821 - photo_disc_loss: 0.7579 - lr: 5.0000e-04 - 178s/epoch - 746ms/step\nEpoch 16/200\n238/238 - 177s - monet_gen_loss: 2.0002 - photo_gen_loss: 1.2702 - monet_disc_loss: 0.5594 - photo_disc_loss: 0.6839 - lr: 5.0000e-04 - 177s/epoch - 745ms/step\nEpoch 17/200\n238/238 - 177s - monet_gen_loss: 1.3748 - photo_gen_loss: 1.2655 - monet_disc_loss: 0.5604 - photo_disc_loss: 0.6788 - lr: 5.0000e-04 - 177s/epoch - 746ms/step\nEpoch 18/200\n238/238 - 177s - monet_gen_loss: 1.4706 - photo_gen_loss: 1.8134 - monet_disc_loss: 0.5409 - photo_disc_loss: 0.3262 - lr: 5.0000e-04 - 177s/epoch - 745ms/step\nEpoch 19/200\n\nEpoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n238/238 - 177s - monet_gen_loss: 1.8316 - photo_gen_loss: 1.9046 - monet_disc_loss: 0.4784 - photo_disc_loss: 0.2618 - lr: 5.0000e-04 - 177s/epoch - 745ms/step\nEpoch 20/200\n238/238 - 177s - monet_gen_loss: 1.2635 - photo_gen_loss: 0.8864 - monet_disc_loss: 0.4958 - photo_disc_loss: 0.6993 - lr: 2.5000e-04 - 177s/epoch - 745ms/step\nEpoch 21/200\n238/238 - 177s - monet_gen_loss: 1.5789 - photo_gen_loss: 1.3511 - monet_disc_loss: 0.5232 - photo_disc_loss: 0.6082 - lr: 2.5000e-04 - 177s/epoch - 745ms/step\nEpoch 22/200\n238/238 - 177s - monet_gen_loss: 1.9444 - photo_gen_loss: 3.3437 - monet_disc_loss: 0.7172 - photo_disc_loss: 0.0584 - lr: 2.5000e-04 - 177s/epoch - 744ms/step\nEpoch 23/200\n238/238 - 177s - monet_gen_loss: 2.4383 - photo_gen_loss: 2.6908 - monet_disc_loss: 0.2536 - photo_disc_loss: 0.2284 - lr: 2.5000e-04 - 177s/epoch - 745ms/step\nEpoch 24/200\n\nEpoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n238/238 - 177s - monet_gen_loss: 2.4699 - photo_gen_loss: 1.6735 - monet_disc_loss: 0.9684 - photo_disc_loss: 0.3230 - lr: 2.5000e-04 - 177s/epoch - 745ms/step\nEpoch 25/200\nCheckpoint guardado: checkpoints/ckpt_epoch_25\n238/238 - 183s - monet_gen_loss: 1.4508 - photo_gen_loss: 2.8558 - monet_disc_loss: 0.5486 - photo_disc_loss: 0.2841 - lr: 1.2500e-04 - 183s/epoch - 769ms/step\nEpoch 26/200\n238/238 - 177s - monet_gen_loss: 2.2511 - photo_gen_loss: 0.8635 - monet_disc_loss: 0.3775 - photo_disc_loss: 0.8367 - lr: 1.2500e-04 - 177s/epoch - 745ms/step\nEpoch 27/200\n238/238 - 177s - monet_gen_loss: 3.1591 - photo_gen_loss: 1.4874 - monet_disc_loss: 0.3960 - photo_disc_loss: 0.7080 - lr: 1.2500e-04 - 177s/epoch - 745ms/step\nEpoch 28/200\n238/238 - 177s - monet_gen_loss: 2.1798 - photo_gen_loss: 4.6756 - monet_disc_loss: 0.5121 - photo_disc_loss: 0.0286 - lr: 1.2500e-04 - 177s/epoch - 745ms/step\nEpoch 29/200\n\nEpoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n238/238 - 177s - monet_gen_loss: 2.0539 - photo_gen_loss: 4.6320 - monet_disc_loss: 0.4733 - photo_disc_loss: 0.0534 - lr: 1.2500e-04 - 177s/epoch - 745ms/step\nEpoch 30/200\n238/238 - 177s - monet_gen_loss: 1.0301 - photo_gen_loss: 7.0765 - monet_disc_loss: 0.7590 - photo_disc_loss: 0.0121 - lr: 6.2500e-05 - 177s/epoch - 744ms/step\nEpoch 31/200\n238/238 - 177s - monet_gen_loss: 2.7834 - photo_gen_loss: 6.2354 - monet_disc_loss: 0.1617 - photo_disc_loss: 0.0073 - lr: 6.2500e-05 - 177s/epoch - 744ms/step\nEpoch 32/200\n238/238 - 177s - monet_gen_loss: 4.4040 - photo_gen_loss: 6.6961 - monet_disc_loss: 0.0917 - photo_disc_loss: 0.9023 - lr: 6.2500e-05 - 177s/epoch - 744ms/step\nEpoch 33/200\n238/238 - 177s - monet_gen_loss: 1.5377 - photo_gen_loss: 1.7077 - monet_disc_loss: 0.5806 - photo_disc_loss: 0.3714 - lr: 6.2500e-05 - 177s/epoch - 745ms/step\nEpoch 34/200\n238/238 - 177s - monet_gen_loss: 1.7405 - photo_gen_loss: 1.1773 - monet_disc_loss: 0.5229 - photo_disc_loss: 0.6415 - lr: 6.2500e-05 - 177s/epoch - 745ms/step\nEpoch 35/200\n\nEpoch 35: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n238/238 - 177s - monet_gen_loss: 1.9591 - photo_gen_loss: 1.2077 - monet_disc_loss: 0.5752 - photo_disc_loss: 0.6439 - lr: 6.2500e-05 - 177s/epoch - 745ms/step\nEpoch 36/200\n238/238 - 177s - monet_gen_loss: 1.1510 - photo_gen_loss: 1.3454 - monet_disc_loss: 0.9112 - photo_disc_loss: 0.6385 - lr: 3.1250e-05 - 177s/epoch - 745ms/step\nEpoch 37/200\n238/238 - 177s - monet_gen_loss: 1.4019 - photo_gen_loss: 1.5620 - monet_disc_loss: 0.6929 - photo_disc_loss: 0.5865 - lr: 3.1250e-05 - 177s/epoch - 745ms/step\nEpoch 38/200\n238/238 - 177s - monet_gen_loss: 2.0786 - photo_gen_loss: 1.3111 - monet_disc_loss: 0.4147 - photo_disc_loss: 0.5825 - lr: 3.1250e-05 - 177s/epoch - 744ms/step\nEpoch 39/200\n238/238 - 177s - monet_gen_loss: 2.4212 - photo_gen_loss: 1.7727 - monet_disc_loss: 0.1512 - photo_disc_loss: 0.5078 - lr: 3.1250e-05 - 177s/epoch - 745ms/step\nEpoch 40/200\n\nEpoch 40: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n238/238 - 177s - monet_gen_loss: 1.7917 - photo_gen_loss: 2.4130 - monet_disc_loss: 0.5523 - photo_disc_loss: 0.2204 - lr: 3.1250e-05 - 177s/epoch - 745ms/step\nEpoch 41/200\n238/238 - 177s - monet_gen_loss: 1.4196 - photo_gen_loss: 4.7744 - monet_disc_loss: 0.5593 - photo_disc_loss: 0.0164 - lr: 1.5625e-05 - 177s/epoch - 744ms/step\nEpoch 42/200\n238/238 - 177s - monet_gen_loss: 1.7773 - photo_gen_loss: 1.5828 - monet_disc_loss: 0.4118 - photo_disc_loss: 0.5823 - lr: 1.5625e-05 - 177s/epoch - 746ms/step\nEpoch 43/200\n238/238 - 177s - monet_gen_loss: 3.1779 - photo_gen_loss: 2.2774 - monet_disc_loss: 0.1910 - photo_disc_loss: 1.8143 - lr: 1.5625e-05 - 177s/epoch - 745ms/step\nEpoch 44/200\n238/238 - 177s - monet_gen_loss: 3.8295 - photo_gen_loss: 1.4325 - monet_disc_loss: 0.0858 - photo_disc_loss: 0.6358 - lr: 1.5625e-05 - 177s/epoch - 745ms/step\nEpoch 45/200\n\nEpoch 45: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n238/238 - 178s - monet_gen_loss: 5.2947 - photo_gen_loss: 3.9574 - monet_disc_loss: 0.2614 - photo_disc_loss: 0.0436 - lr: 1.5625e-05 - 178s/epoch - 748ms/step\nEpoch 46/200\n238/238 - 178s - monet_gen_loss: 1.7323 - photo_gen_loss: 3.6490 - monet_disc_loss: 0.3775 - photo_disc_loss: 0.1262 - lr: 7.8125e-06 - 178s/epoch - 746ms/step\nEpoch 47/200\n238/238 - 178s - monet_gen_loss: 2.1851 - photo_gen_loss: 2.0849 - monet_disc_loss: 0.2049 - photo_disc_loss: 0.2620 - lr: 7.8125e-06 - 178s/epoch - 746ms/step\nEpoch 48/200\n238/238 - 178s - monet_gen_loss: 1.4551 - photo_gen_loss: 1.2920 - monet_disc_loss: 0.6244 - photo_disc_loss: 0.6626 - lr: 7.8125e-06 - 178s/epoch - 747ms/step\nEpoch 49/200\n238/238 - 178s - monet_gen_loss: 3.1572 - photo_gen_loss: 2.0585 - monet_disc_loss: 0.9056 - photo_disc_loss: 0.7320 - lr: 7.8125e-06 - 178s/epoch - 747ms/step\nEpoch 50/200\n\nEpoch 50: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\nCheckpoint guardado: checkpoints/ckpt_epoch_50\n238/238 - 184s - monet_gen_loss: 3.4520 - photo_gen_loss: 3.8852 - monet_disc_loss: 0.1471 - photo_disc_loss: 0.0482 - lr: 7.8125e-06 - 184s/epoch - 771ms/step\nEpoch 51/200\n238/238 - 178s - monet_gen_loss: 3.4783 - photo_gen_loss: 5.0767 - monet_disc_loss: 0.3733 - photo_disc_loss: 0.0171 - lr: 3.9063e-06 - 178s/epoch - 747ms/step\nEpoch 52/200\n238/238 - 178s - monet_gen_loss: 1.3438 - photo_gen_loss: 2.4008 - monet_disc_loss: 0.6102 - photo_disc_loss: 0.5949 - lr: 3.9063e-06 - 178s/epoch - 746ms/step\nEpoch 53/200\n238/238 - 178s - monet_gen_loss: 2.5100 - photo_gen_loss: 1.6010 - monet_disc_loss: 0.1292 - photo_disc_loss: 0.6333 - lr: 3.9063e-06 - 178s/epoch - 747ms/step\nEpoch 54/200\n238/238 - 177s - monet_gen_loss: 1.9206 - photo_gen_loss: 3.3875 - monet_disc_loss: 0.5691 - photo_disc_loss: 0.0987 - lr: 3.9063e-06 - 177s/epoch - 746ms/step\nEpoch 55/200\n\nEpoch 55: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n238/238 - 178s - monet_gen_loss: 1.5232 - photo_gen_loss: 3.7525 - monet_disc_loss: 0.6988 - photo_disc_loss: 0.5006 - lr: 3.9063e-06 - 178s/epoch - 746ms/step\nEpoch 56/200\n238/238 - 178s - monet_gen_loss: 2.2841 - photo_gen_loss: 4.7489 - monet_disc_loss: 0.3855 - photo_disc_loss: 0.0164 - lr: 1.9531e-06 - 178s/epoch - 746ms/step\nEpoch 57/200\n238/238 - 177s - monet_gen_loss: 4.3329 - photo_gen_loss: 7.0116 - monet_disc_loss: 0.0374 - photo_disc_loss: 0.0122 - lr: 1.9531e-06 - 177s/epoch - 746ms/step\nEpoch 58/200\n238/238 - 178s - monet_gen_loss: 3.3657 - photo_gen_loss: 5.6899 - monet_disc_loss: 0.2240 - photo_disc_loss: 0.0084 - lr: 1.9531e-06 - 178s/epoch - 746ms/step\nEpoch 59/200\n238/238 - 178s - monet_gen_loss: 4.0134 - photo_gen_loss: 2.5993 - monet_disc_loss: 0.0591 - photo_disc_loss: 0.2647 - lr: 1.9531e-06 - 178s/epoch - 746ms/step\nEpoch 60/200\n\nEpoch 60: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n238/238 - 178s - monet_gen_loss: 2.0413 - photo_gen_loss: 5.0031 - monet_disc_loss: 0.8356 - photo_disc_loss: 0.0173 - lr: 1.9531e-06 - 178s/epoch - 746ms/step\nEpoch 61/200\n238/238 - 178s - monet_gen_loss: 2.9878 - photo_gen_loss: 5.7109 - monet_disc_loss: 0.2094 - photo_disc_loss: 0.0094 - lr: 9.7656e-07 - 178s/epoch - 748ms/step\nEpoch 62/200\n238/238 - 177s - monet_gen_loss: 2.9348 - photo_gen_loss: 1.4324 - monet_disc_loss: 0.3745 - photo_disc_loss: 0.6844 - lr: 9.7656e-07 - 177s/epoch - 746ms/step\nEpoch 63/200\n238/238 - 178s - monet_gen_loss: 3.7654 - photo_gen_loss: 1.4700 - monet_disc_loss: 0.4730 - photo_disc_loss: 0.6851 - lr: 9.7656e-07 - 178s/epoch - 747ms/step\nEpoch 64/200\n238/238 - 177s - monet_gen_loss: 2.5270 - photo_gen_loss: 1.1983 - monet_disc_loss: 0.3472 - photo_disc_loss: 0.7628 - lr: 9.7656e-07 - 177s/epoch - 746ms/step\nEpoch 65/200\n\nEpoch 65: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n238/238 - 178s - monet_gen_loss: 1.7495 - photo_gen_loss: 1.4068 - monet_disc_loss: 0.5357 - photo_disc_loss: 0.6641 - lr: 9.7656e-07 - 178s/epoch - 746ms/step\nEpoch 66/200\n238/238 - 178s - monet_gen_loss: 3.1001 - photo_gen_loss: 1.5640 - monet_disc_loss: 0.2479 - photo_disc_loss: 0.7208 - lr: 4.8828e-07 - 178s/epoch - 747ms/step\nEpoch 67/200\n238/238 - 178s - monet_gen_loss: 5.0006 - photo_gen_loss: 2.2119 - monet_disc_loss: 0.0273 - photo_disc_loss: 0.7040 - lr: 4.8828e-07 - 178s/epoch - 747ms/step\nEpoch 68/200\n238/238 - 177s - monet_gen_loss: 2.3663 - photo_gen_loss: 3.8884 - monet_disc_loss: 0.7726 - photo_disc_loss: 0.0477 - lr: 4.8828e-07 - 177s/epoch - 746ms/step\nEpoch 69/200\n238/238 - 178s - monet_gen_loss: 1.1961 - photo_gen_loss: 5.2320 - monet_disc_loss: 0.6271 - photo_disc_loss: 0.0088 - lr: 4.8828e-07 - 178s/epoch - 747ms/step\nEpoch 70/200\n\nEpoch 70: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n238/238 - 178s - monet_gen_loss: 2.2555 - photo_gen_loss: 2.3108 - monet_disc_loss: 0.4031 - photo_disc_loss: 0.2368 - lr: 4.8828e-07 - 178s/epoch - 747ms/step\nEpoch 71/200\n238/238 - 178s - monet_gen_loss: 2.5667 - photo_gen_loss: 1.3091 - monet_disc_loss: 0.3599 - photo_disc_loss: 0.4480 - lr: 2.4414e-07 - 178s/epoch - 746ms/step\nEpoch 72/200\n238/238 - 178s - monet_gen_loss: 2.5438 - photo_gen_loss: 4.4322 - monet_disc_loss: 0.2234 - photo_disc_loss: 0.0255 - lr: 2.4414e-07 - 178s/epoch - 746ms/step\nEpoch 73/200\n238/238 - 178s - monet_gen_loss: 1.4840 - photo_gen_loss: 5.4664 - monet_disc_loss: 0.5002 - photo_disc_loss: 0.0286 - lr: 2.4414e-07 - 178s/epoch - 747ms/step\nEpoch 74/200\n238/238 - 178s - monet_gen_loss: 2.1300 - photo_gen_loss: 2.2215 - monet_disc_loss: 0.3132 - photo_disc_loss: 0.3592 - lr: 2.4414e-07 - 178s/epoch - 747ms/step\nEpoch 75/200\n\nEpoch 75: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\nCheckpoint guardado: checkpoints/ckpt_epoch_75\n238/238 - 184s - monet_gen_loss: 1.7431 - photo_gen_loss: 1.1725 - monet_disc_loss: 0.3685 - photo_disc_loss: 0.6071 - lr: 2.4414e-07 - 184s/epoch - 771ms/step\nEpoch 76/200\n238/238 - 178s - monet_gen_loss: 1.3242 - photo_gen_loss: 1.2419 - monet_disc_loss: 0.5471 - photo_disc_loss: 0.6654 - lr: 1.2207e-07 - 178s/epoch - 746ms/step\nEpoch 77/200\n238/238 - 177s - monet_gen_loss: 2.6538 - photo_gen_loss: 1.4376 - monet_disc_loss: 0.2599 - photo_disc_loss: 0.7399 - lr: 1.2207e-07 - 177s/epoch - 745ms/step\nEpoch 78/200\n238/238 - 178s - monet_gen_loss: 2.3517 - photo_gen_loss: 3.7305 - monet_disc_loss: 0.4263 - photo_disc_loss: 0.0491 - lr: 1.2207e-07 - 178s/epoch - 746ms/step\nEpoch 79/200\n238/238 - 178s - monet_gen_loss: 2.0411 - photo_gen_loss: 1.3766 - monet_disc_loss: 0.6196 - photo_disc_loss: 0.6663 - lr: 1.2207e-07 - 178s/epoch - 747ms/step\nEpoch 80/200\n\nEpoch 80: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n238/238 - 177s - monet_gen_loss: 5.1898 - photo_gen_loss: 4.8844 - monet_disc_loss: 0.0290 - photo_disc_loss: 0.0268 - lr: 1.2207e-07 - 177s/epoch - 746ms/step\nEpoch 81/200\n238/238 - 177s - monet_gen_loss: 1.2876 - photo_gen_loss: 1.9670 - monet_disc_loss: 0.7915 - photo_disc_loss: 0.7084 - lr: 6.1035e-08 - 177s/epoch - 746ms/step\nEpoch 82/200\n238/238 - 178s - monet_gen_loss: 2.1054 - photo_gen_loss: 5.4804 - monet_disc_loss: 0.2367 - photo_disc_loss: 0.0443 - lr: 6.1035e-08 - 178s/epoch - 746ms/step\nEpoch 83/200\n238/238 - 178s - monet_gen_loss: 1.7765 - photo_gen_loss: 1.8953 - monet_disc_loss: 0.4485 - photo_disc_loss: 0.3203 - lr: 6.1035e-08 - 178s/epoch - 746ms/step\nEpoch 84/200\n238/238 - 177s - monet_gen_loss: 5.9056 - photo_gen_loss: 4.8608 - monet_disc_loss: 0.0881 - photo_disc_loss: 0.0432 - lr: 6.1035e-08 - 177s/epoch - 745ms/step\nEpoch 85/200\n\nEpoch 85: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n238/238 - 178s - monet_gen_loss: 1.3696 - photo_gen_loss: 5.7253 - monet_disc_loss: 1.0133 - photo_disc_loss: 0.3286 - lr: 6.1035e-08 - 178s/epoch - 747ms/step\nEpoch 86/200\n238/238 - 178s - monet_gen_loss: 1.3631 - photo_gen_loss: 1.0078 - monet_disc_loss: 0.5806 - photo_disc_loss: 0.7504 - lr: 3.0518e-08 - 178s/epoch - 746ms/step\nEpoch 87/200\n238/238 - 177s - monet_gen_loss: 2.5611 - photo_gen_loss: 1.5902 - monet_disc_loss: 0.4653 - photo_disc_loss: 0.8009 - lr: 3.0518e-08 - 177s/epoch - 746ms/step\nEpoch 88/200\n238/238 - 178s - monet_gen_loss: 3.7272 - photo_gen_loss: 4.8636 - monet_disc_loss: 0.1982 - photo_disc_loss: 0.0471 - lr: 3.0518e-08 - 178s/epoch - 746ms/step\nEpoch 89/200\n238/238 - 178s - monet_gen_loss: 7.2740 - photo_gen_loss: 5.0295 - monet_disc_loss: 0.0114 - photo_disc_loss: 0.9928 - lr: 3.0518e-08 - 178s/epoch - 746ms/step\nEpoch 90/200\n\nEpoch 90: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n238/238 - 178s - monet_gen_loss: 1.5786 - photo_gen_loss: 1.7012 - monet_disc_loss: 0.8201 - photo_disc_loss: 0.6398 - lr: 3.0518e-08 - 178s/epoch - 746ms/step\nEpoch 91/200\n238/238 - 177s - monet_gen_loss: 5.1489 - photo_gen_loss: 3.0308 - monet_disc_loss: 0.0644 - photo_disc_loss: 0.1310 - lr: 1.5259e-08 - 177s/epoch - 745ms/step\nEpoch 92/200\n238/238 - 178s - monet_gen_loss: 2.9928 - photo_gen_loss: 1.2978 - monet_disc_loss: 0.4094 - photo_disc_loss: 0.6518 - lr: 1.5259e-08 - 178s/epoch - 746ms/step\nEpoch 93/200\n238/238 - 177s - monet_gen_loss: 2.3514 - photo_gen_loss: 1.5726 - monet_disc_loss: 0.1781 - photo_disc_loss: 0.5638 - lr: 1.5259e-08 - 177s/epoch - 746ms/step\nEpoch 94/200\n238/238 - 177s - monet_gen_loss: 4.3411 - photo_gen_loss: 1.6256 - monet_disc_loss: 0.0279 - photo_disc_loss: 0.6654 - lr: 1.5259e-08 - 177s/epoch - 745ms/step\nEpoch 95/200\n\nEpoch 95: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n238/238 - 178s - monet_gen_loss: 5.8912 - photo_gen_loss: 1.7699 - monet_disc_loss: 0.0108 - photo_disc_loss: 0.8683 - lr: 1.5259e-08 - 178s/epoch - 746ms/step\nEpoch 96/200\n238/238 - 178s - monet_gen_loss: 6.4224 - photo_gen_loss: 5.7955 - monet_disc_loss: 0.0161 - photo_disc_loss: 0.0079 - lr: 7.6294e-09 - 178s/epoch - 746ms/step\nEpoch 97/200\n238/238 - 178s - monet_gen_loss: 3.8189 - photo_gen_loss: 1.3252 - monet_disc_loss: 0.0680 - photo_disc_loss: 0.9064 - lr: 7.6294e-09 - 178s/epoch - 746ms/step\nEpoch 98/200\n238/238 - 178s - monet_gen_loss: 3.8021 - photo_gen_loss: 1.4136 - monet_disc_loss: 0.1223 - photo_disc_loss: 0.5397 - lr: 7.6294e-09 - 178s/epoch - 746ms/step\nEpoch 99/200\n238/238 - 178s - monet_gen_loss: 2.2646 - photo_gen_loss: 1.7334 - monet_disc_loss: 0.3150 - photo_disc_loss: 0.4069 - lr: 7.6294e-09 - 178s/epoch - 747ms/step\nEpoch 100/200\n\nEpoch 100: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\nCheckpoint guardado: checkpoints/ckpt_epoch_100\n238/238 - 183s - monet_gen_loss: 2.0119 - photo_gen_loss: 2.6632 - monet_disc_loss: 0.3315 - photo_disc_loss: 0.1035 - lr: 7.6294e-09 - 183s/epoch - 770ms/step\nEpoch 101/200\n238/238 - 178s - monet_gen_loss: 6.9958 - photo_gen_loss: 1.9671 - monet_disc_loss: 0.0202 - photo_disc_loss: 0.4160 - lr: 3.8147e-09 - 178s/epoch - 746ms/step\nEpoch 102/200\n238/238 - 177s - monet_gen_loss: 6.7817 - photo_gen_loss: 4.4821 - monet_disc_loss: 0.0069 - photo_disc_loss: 0.0704 - lr: 3.8147e-09 - 177s/epoch - 745ms/step\nEpoch 103/200\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# =====================================\n# Celda 3: REANUDAR ENTRENAMIENTO\n# =====================================\n\n# Volvemos a usar las mismas definiciones de generator_fn(), discriminator_fn(), etc.\n\nwith strategy.scope():\n    # 1. Crear redes con la misma arquitectura\n    monet_generator_resumed = generator_fn()\n    photo_generator_resumed = generator_fn()\n    monet_discriminator_resumed = discriminator_fn()\n    photo_discriminator_resumed = discriminator_fn()\n\n    # 2. Crear los mismos optimizadores\n    monet_generator_optimizer_resumed = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer_resumed = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    monet_discriminator_optimizer_resumed = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer_resumed = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    # 3. Instanciar un nuevo CycleGAN\n    gan_model_resumed = CycleGan(\n        monet_generator_resumed,\n        photo_generator_resumed,\n        monet_discriminator_resumed,\n        photo_discriminator_resumed,\n        lambda_cycle=10\n    )\n    gan_model_resumed.compile(\n        m_gen_optimizer=monet_generator_optimizer_resumed,\n        p_gen_optimizer=photo_generator_optimizer_resumed,\n        m_disc_optimizer=monet_discriminator_optimizer_resumed,\n        p_disc_optimizer=photo_discriminator_optimizer_resumed,\n        gen_loss_fn=generator_loss,\n        disc_loss_fn=discriminator_loss,\n        cycle_loss_fn=calc_cycle_loss,\n        identity_loss_fn=identity_loss,\n        perceptual_loss_fn=perceptual_loss,\n        layers_dict=layers_dict,\n        style_weights=style_weights,\n        perceptual_lambda=0.1\n    )\n\n    # 4. Crear un tf.train.Checkpoint para restaurar estado (G, D y Opts)\n    ckpt_resumed = tf.train.Checkpoint(\n        monet_generator=gan_model_resumed.m_gen,\n        photo_generator=gan_model_resumed.p_gen,\n        monet_discriminator=gan_model_resumed.m_disc,\n        photo_discriminator=gan_model_resumed.p_disc,\n        monet_gen_optimizer=gan_model_resumed.m_gen_optimizer,\n        photo_gen_optimizer=gan_model_resumed.p_gen_optimizer,\n        monet_disc_optimizer=gan_model_resumed.m_disc_optimizer,\n        photo_disc_optimizer=gan_model_resumed.p_disc_optimizer\n    )\n\n    # 5. Indica el checkpoint a cargar (ej. el de la época 30)\n    #    asumiendo que lo guardó como: checkpoints/ckpt_epoch_30-1\n    #    (o similar, usa tf.train.latest_checkpoint para encontrarlo)\n    checkpoint_dir = '/kaggle/input/checkpoint30-diffusse-myopia' \n    latest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n    if latest_ckpt:\n        ckpt_resumed.restore(latest_ckpt).expect_partial()\n        print(f\"Checkpoint restaurado desde {latest_ckpt}\")\n    else:\n        print(\"No se encontró checkpoint en la carpeta:\", checkpoint_dir)\n\n    # 6. Callbacks para seguir entrenando\n    display_callback = DisplaySampleCallback(\n        image_path='/kaggle/input/dataset-normal-1024/normal/002c2135.jpg',  # Replace with your image path\n        monet_generator=gan_model.m_gen,     # Your Monet generator model\n        output_dir='./sample_epochs',        # Directory to save generated images\n        freq=1                               # Frequency of saving images (every epoch)\n    )\n    reduce_lr_callback_resumed = ReduceLROnPlateau(\n        monitor='monet_gen_loss',\n        factor=0.5,\n        patience=5,\n        verbose=1,\n        mode='min'\n    )\n    checkpoint_callback_resumed = CustomTrainCheckpoints(\n        checkpoint_dir='checkpoints_resumed',  # carpeta distinta o la misma\n        save_freq=25\n    )\n\n    # 7. Reanudar entrenamiento: si lo dejaste en la 30, pon initial_epoch=30\n    #    para que la siguiente sea la 31\n    history_resumed = gan_model_resumed.fit(\n        gan_ds,\n        steps_per_epoch=steps_per_epoch,\n        epochs=60,         # entrenar otras 30 epocas, por ejemplo\n        initial_epoch=30,  # arranca contaje en 30\n        verbose=2,\n        callbacks=[display_callback_resumed, reduce_lr_callback_resumed, checkpoint_callback_resumed]\n    )\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\np_gen_checkpoint = '/kaggle/input/modelos-epoca38-myopia/gan_model_myopiaNoAttention_epoch_38.h5'\nm_gen_checkpoint = '/kaggle/working/model_checkpoints/gan_model_myopiaNoAttention_epoch_38.h5'\n\nprint(f\"¿Existe {p_gen_checkpoint}? {os.path.exists(p_gen_checkpoint)}\")\nprint(f\"¿Existe {m_gen_checkpoint}? {os.path.exists(m_gen_checkpoint)}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gan_model.p_gen.save('gan_model_normal_myopiaNoAttention66.h5')  # Guarda el generador que transforma fotos en Monet","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gan_model.m_gen.save('gan_model_myopiaNoAttention66.h5')  # Guarda el generador que transforma fotos en Monet","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport zipfile\n\ndef folder_to_zip(folder_path, output_zip_path):\n    # Crear un archivo .zip\n    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Recorrer todos los archivos en el directorio\n        for root, dirs, files in os.walk(folder_path):\n            for file in files:\n                # Crear la ruta completa del archivo\n                full_path = os.path.join(root, file)\n                # Agregar el archivo al zip, eliminando el prefijo de la carpeta original\n                zipf.write(full_path, os.path.relpath(full_path, folder_path))\n\n    print(f\"Carpeta {folder_path} comprimida en {output_zip_path}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folder_to_zip('/kaggle/working/checkpoints', 'checkpointsnocnnpatchy100.zip')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folder_to_zip('/kaggle/working/sample_epochs', 'sample_epochs_resumednocnnpatchy100.zip')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history.keys()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suponiendo que tienes un objeto de historial que contiene los datos de entrenamiento\nfor key in history.keys():\n    data = np.array(history[key])  # Asegúrate de que 'history[key]' tiene la forma correcta\n    # Verificar la forma de los datos\n    print(key, data.shape)\n\n    # Si los datos son de alta dimensión, podríamos promediarlos\n    # Asegúrate de que este enfoque tiene sentido para tus datos específicos\n    if data.ndim > 2:\n        data = data.mean(axis=tuple(range(1, data.ndim)))\n\n    plt.plot(data, label=key)\n    \n\nplt.title('Curvas de Pérdida durante el Entrenamiento')\nplt.xlabel('Épocas')\nplt.ylabel('Pérdida')\nplt.legend()\nplt.show() ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Suponiendo que tienes un objeto de historial que contiene los datos de entrenamiento\nfor key in history.keys():\n    data = np.array(history[key])  # Asegúrate de que 'history[key]' tiene la forma correcta\n    # Verificar la forma de los datos\n    print(key, data.shape)\n\n    # Si los datos son de alta dimensión, podríamos promediarlos\n    # Asegúrate de que este enfoque tiene sentido para tus datos específicos\n    if data.ndim > 2:\n        data = data.mean(axis=tuple(range(1, data.ndim)))\n\n    plt.plot(data, label=key)\n\nplt.title('Curvas de Pérdida durante el Entrenamiento')\nplt.xlabel('Épocas')\nplt.ylabel('Pérdida')\nplt.legend()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_generated_samples(load_dataset('class_1.tfrecord').batch(1), monet_generator, 25)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_generated_samples(load_dataset('class_2.tfrecord').batch(1), photo_generator, 25)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs('../images/') # Create folder to save generated images\n\npredict_and_save(load_dataset(class_normal_filenames).batch(1), monet_generator, '../images/')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shutil.make_archive('/kaggle/working/images/', 'zip', '../images')\n\nprint(f\"Generated samples: {len([name for name in os.listdir('../images/') if os.path.isfile(os.path.join('../images/', name))])}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Definiciones globales\nHEIGHT = 512\nWIDTH = 512\nCHANNELS = 3\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Función para decodificar y preprocesar imágenes\ndef decode_image(image):\n    image = tf.image.decode_png(image, channels=CHANNELS)\n    image = tf.image.resize(image, [HEIGHT, WIDTH])\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    return image\n\ndef load_image(filename):\n    image = tf.io.read_file(filename)\n    image = decode_image(image)\n    return image\n\ndef load_dataset(filenames):\n    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n    dataset = dataset.map(load_image, num_parallel_calls=AUTO)\n    return dataset\n\n\nimport tensorflow_addons as tfa  # Importa tensorflow_addons\n\n\n\n# Función para hacer predicciones\ndef predict_image(model, image_path):\n    image = load_image(image_path)\n    image = tf.expand_dims(image, axis=0)  # Añadir la dimensión de batch\n    prediction = model.predict(image)\n    prediction = (prediction + 1) * 0.5  # Transformar de [-1, 1] a [0, 1]\n    return prediction\n# helper function for un-normalizing an image \n# and converting it from a Tensor image to a NumPy image for display\ndef im_convert(tensor):\n    \"\"\" Display a tensor as an image. \"\"\"\n    \n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.numpy().squeeze()\n    image = image.transpose(1,2,0)\n    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n    image = image.clip(0, 1)\n\n    return image\n# Visualización de una imagen de predicción\n\n\n\ndef display_prediction(image_path):\n    plt.figure(figsize=(6, 6))\n    prediction = predict_image(gan_model.m_gen, image_path)\n    \n    # Remove batch dimension if necessary\n    prediction = prediction.squeeze()  # Remove batch dimension\n\n    # Clip values to the expected range [0, 1] if not already\n    prediction = np.clip(prediction, 0, 1)\n\n    # Convert to 8-bit per channel format for image representation\n    prediction = (prediction * 255).astype(np.uint8)\n\n    # Display using matplotlib\n    plt.imshow(prediction)\n    plt.axis('off')\n    plt.show()\n\n    # Save using PIL\n    img = Image.fromarray(prediction)\n    img.save(\"reconstruccion2.png\")  # Saving as PNG\n\n# Use the function with an appropriate image path\nimage_path = '/kaggle/input/aptos-crop/train_images_crop/0c917c372572.png'\ndisplay_prediction(image_path)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = '/kaggle/input/aptos-crop/train_images_crop/0e0fc1d9810c.png'\ndisplay_prediction(image_path)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = '/kaggle/input/aptos-crop/train_images_crop/0a4e1a29ffff.png'\ndisplay_prediction(image_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = '/kaggle/input/aptos-crop/train_images_crop/1409ab48175a.png'\ndisplay_prediction(image_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-25T03:35:20.309Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Use the function with an appropriate image path\nimage_path = '/kaggle/input/aptos-crop/train_images_crop/02da652c74b8.png'\ndisplay_prediction(image_path)\n","metadata":{}}]}